# -*- coding: utf-8 -*-
"""qa_agent_ocean_ai.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TAJISXaLMTrfCOyg9Y9VbmuLpSoA7UUY
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture no-stderr
# !pip install -qU langgraph langchain langchain_community langchain_openai langchain_google_genai
# !pip install -qU langchain-huggingface sentence-transformers
# !pip install pinecone langchain-pinecone
# !pip install pypdf unstructured

import os
from typing import TypedDict, List, Optional
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
from langchain_openai import ChatOpenAI
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Pinecone as LangChainPinecone
from pinecone import Pinecone, ServerlessSpec
from langchain_pinecone import PineconeVectorStore
from langgraph.graph import StateGraph, END
from google.colab import userdata


# --- API KEYS ---
os.environ["OPENAI_API_KEY"] = userdata.get('OPENAI_API_KEY')
os.environ["GOOGLE_API_KEY"] = userdata.get('GOOGLE_API_KEY')
os.environ["PINECONE_API_KEY"] = userdata.get('PINECONE_API_KEY')
os.environ["HF_TOKEN"] = userdata.get('HF_TOKEN')

# --- MODEL SETUP ---
# 1. Gemini 2.5 Flash (Fast, huge context for RAG & Test Cases)
llm_gemini = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0)

# 2. GPT-4o (Strict, reliable for Selenium Code)
llm_openai = ChatOpenAI(model="gpt-4o", temperature=0)

# 3. Embeddings & Vector Store
embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
pc = Pinecone(api_key=os.environ["PINECONE_API_KEY"])
index_name = "qa-agent-index"

if not pc.has_index(index_name):
    pc.create_index(
        name=index_name,
        vector_type="dense",
        dimension=384,
        metric="cosine",
        spec=ServerlessSpec(
            cloud="aws",
            region="us-east-1"
        )
    )

index = pc.Index(index_name)
vector_store = PineconeVectorStore(index=index, embedding=embeddings)

from typing import TypedDict, Optional, List

class QAState(TypedDict):
    current_kb_id: Optional[str]
    user_query: Optional[str]
    retrieved_docs: Optional[List[str]]
    test_cases: Optional[str]
    selected_test_case: Optional[str]
    html_content: Optional[str]
    raw_documents: Optional[list]
    selanium_script:Optional[str]

# ---------- GLOBAL REGISTRY ----------
KB_REGISTRY = {}

GLOBAL_STATE = {
    "current_kb_id": None,
    "user_query": None,
    "retrieved_docs": None,
    "test_cases": None,
    "selected_test_case": None,
    "html_content": None,
    "raw_documents": None,
    "selanium_script":None
}

import uuid
from langchain_text_splitters import RecursiveCharacterTextSplitter

def build_kb_node(state: QAState):
    if not state.get("raw_documents"):
        raise ValueError("No raw documents provided to build knowledge base")

    raw_documents = state["raw_documents"]
    kb_id = f"kb_{uuid.uuid4().hex}"

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=800,
        chunk_overlap=150
    )

    chunks = splitter.split_documents(raw_documents)

    PineconeVectorStore.from_documents(
        documents=chunks,
        embedding=embeddings,
        index_name=index_name
    )

    KB_REGISTRY[kb_id] = vector_store
    state["current_kb_id"] = kb_id

    return state

import os
import json
from langchain_core.documents import Document
from langchain_community.document_loaders import PyPDFLoader, TextLoader, UnstructuredFileLoader

def load_any_document(file_path: str):
    ext = file_path.split('.')[-1].lower()

    if ext == "pdf":
        loader = PyPDFLoader(file_path)
        return loader.load()

    elif ext in ["txt", "md"]:
        loader = TextLoader(file_path, encoding="utf-8")
        return loader.load()

    elif ext == "json":
        with open(file_path, "r", encoding="utf-8") as f:
            data = json.load(f)

        text = json.dumps(data, indent=2)
        return [Document(
            page_content=text,
            metadata={"source_document": os.path.basename(file_path)}
        )]

    else:
        loader = UnstructuredFileLoader(file_path)
        return loader.load()

def load_documents_bulk(file_paths):
    all_docs = []
    for path in file_paths:
        docs = load_any_document(path)
        all_docs.extend(docs)
    return all_docs

files = [
    "/content/api_endpoints.json",
    "/content/product_specs.md",
    "/content/ui_ux_guide.txt",

]

raw_documents = load_documents_bulk(files)
GLOBAL_STATE["raw_documents"] = raw_documents
print(f"Loaded {len(raw_documents)} documents")

GLOBAL_STATE = build_kb_node(GLOBAL_STATE)
print("✅ KB Active:", GLOBAL_STATE["current_kb_id"])

print("Registered KBs:", KB_REGISTRY.keys())
print("Active KB:", GLOBAL_STATE["current_kb_id"])

def ensure_kb_exists(state):
    if not state.get("current_kb_id"):
        raise ValueError("❌ No active Knowledge Base. Please build KB first.")
    return state

def retrieve_relevant_docs(state):
    vectordb = KB_REGISTRY[state["current_kb_id"]]

    results = vectordb.similarity_search(
        state["user_query"],
        k=5
    )

    state["retrieved_docs"] = [
        doc.page_content + f"\nSOURCE: {doc.metadata.get('source_document')}"
        for doc in results
    ]
    return state

import json
import re

def convert_testcases_to_dict(raw_text: str):
    """
    Converts markdown-wrapped JSON test cases into a Python list/dict object
    """

    # 1. Remove heading like ====== TEST CASES ======
    cleaned = re.sub(r"=+ TEST CASES =+", "", raw_text, flags=re.IGNORECASE).strip()

    # 2. Extract JSON block inside ```json ... ```
    match = re.search(r"```json(.*?)```", cleaned, re.DOTALL)

    if match:
        json_text = match.group(1).strip()
    else:
        json_text = cleaned.strip()

    # 3. Convert to Python object
    try:
        return json.loads(json_text)
    except json.JSONDecodeError as e:
        raise ValueError(f"Invalid JSON format: {e}")

def generate_test_cases_node(state):
    context = "\n\n".join(state["retrieved_docs"])

    prompt = f"""
You are an expert QA Engineer.

Use ONLY the information provided below. Do NOT assume anything outside it.

Context:
{context}

User Requirement:
{state['user_query']}

Generate structured test cases in JSON with fields:
- Test_ID
- Feature
- Test_Scenario
- Expected_Result
- Grounded_In

Ensure every test case mentions its source.
"""

    response = llm_gemini.invoke([HumanMessage(content=prompt)])
    state["test_cases"] = response.content
    state["test_cases"] = convert_testcases_to_dict(state["test_cases"])
    return state

from langgraph.graph import StateGraph, END
from IPython.display import display, Image
tc_graph = StateGraph(dict)

tc_graph.add_node("check_kb", ensure_kb_exists)
tc_graph.add_node("retrieve_docs", retrieve_relevant_docs)
tc_graph.add_node("generate_test_cases", generate_test_cases_node)

tc_graph.set_entry_point("check_kb")
tc_graph.add_edge("check_kb", "retrieve_docs")
tc_graph.add_edge("retrieve_docs", "generate_test_cases")
tc_graph.add_edge("generate_test_cases", END)

tc_executor = tc_graph.compile()
display(Image(tc_executor.get_graph().draw_mermaid_png()))

GLOBAL_STATE["user_query"] = "Generate positive and negative test cases for discount code feature"

GLOBAL_STATE = tc_executor.invoke(GLOBAL_STATE)

print("====== GENERATED TEST CASES ======")
print(GLOBAL_STATE["test_cases"])

def load_html_file(file_path: str):
    with open(file_path, "r", encoding="utf-8") as f:
        return f.read()

import re

def extract_python_script(llm_response: str) -> str:
    """
    Converts LLM response into a clean, executable Python Selenium script
    """

    # 1. Try to extract code inside ```python blocks
    code_block = re.search(r"```(?:python)?(.*?)```", llm_response, re.DOTALL)

    if code_block:
        script = code_block.group(1).strip()
    else:
        script = llm_response.strip()

    # 2. Remove leading/trailing commentary lines
    lines = script.splitlines()
    cleaned_lines = []

    for line in lines:
        if line.strip().startswith("Explanation") or line.strip().startswith("Here"):
            continue
        cleaned_lines.append(line)

    script = "\n".join(cleaned_lines)

    # 3. Guarantee UTF-8 encoding header & execution guard
    final_script = f"""# -*- coding: utf-8 -*-
{script}

if __name__ == "__main__":
    pass
"""

    return final_script

def generate_selenium_node(state):
    if not state.get("selected_test_case"):
        raise ValueError("No test case selected for Selenium generation.")

    prompt = f"""
You are a Selenium Python expert.

Convert the following test case into a FULLY EXECUTABLE Selenium script.

Test Case:
{state["selected_test_case"]}

HTML:
{state["html_content"]}

Rules:
- Use correct CSS/ID/XPath selectors from HTML
- Use Selenium WebDriver (Python)
- Include setup and teardown
- Script must be runnable
- Handle waits properly (WebDriverWait)

Return ONLY Python code.
"""

    response = llm_openai.invoke([HumanMessage(content=prompt)])
    state["selanium_script"] = response.content
    state["selanium_script"] = extract_python_script(state["selanium_script"])
    return state

selenium_graph = StateGraph(dict)

selenium_graph.add_node("generate_selenium", generate_selenium_node)
selenium_graph.set_entry_point("generate_selenium")
selenium_graph.add_edge("generate_selenium", END)

selenium_executor = selenium_graph.compile()
display(Image(selenium_executor.get_graph().draw_mermaid_png()))

selected_test_case = """
Test_ID: TC-001
Feature: Discount Code
Test_Scenario: Apply valid discount code SAVE15
Expected_Result: Total price reduced by 15%
Grounded_In: product_specs.md
"""

GLOBAL_STATE["selected_test_case"] = selected_test_case
GLOBAL_STATE["html_content"] = load_html_file("/content/checkout.html")
GLOBAL_STATE = selenium_executor.invoke(GLOBAL_STATE)


print("====== CLEAN SELENIUM SCRIPT ======")
print(GLOBAL_STATE["selenium_script"])

def auto_select_test_case(state):
    if not state.get("test_cases"):
        raise ValueError("No test cases available")

    state["selected_test_case"] = state["test_cases"][0]
    return state

final_graph = StateGraph(QAState)

final_graph.add_node("build_kb", build_kb_node)
final_graph.add_node("check_kb", ensure_kb_exists)
final_graph.add_node("retrieve_docs", retrieve_relevant_docs)
final_graph.add_node("generate_test_cases", generate_test_cases_node)
final_graph.add_node("select_test_case", auto_select_test_case)
final_graph.add_node("generate_selenium", generate_selenium_node)

final_graph.set_entry_point("build_kb")

final_graph.add_edge("build_kb", "check_kb")
final_graph.add_edge("check_kb", "retrieve_docs")
final_graph.add_edge("retrieve_docs", "generate_test_cases")
final_graph.add_edge("generate_test_cases", "select_test_case")
final_graph.add_edge("select_test_case", "generate_selenium")
final_graph.add_edge("generate_selenium", END)

final_executor = final_graph.compile()
display(Image(final_executor.get_graph().draw_mermaid_png()))

FINAL_RESULT = final_executor.invoke(GLOBAL_STATE)

FINAL_RESULT.keys()

print("====== TEST CASES ======")
print(FINAL_RESULT["test_cases"])

print("\n====== SELENIUM SCRIPT ======")
print(FINAL_RESULT["selanium_script"])

